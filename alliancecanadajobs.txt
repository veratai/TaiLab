#using Digital Research of Alliance Canada (Compute Canada) resources

# for help e-mail support@tech.alliancecan.ca


# should connect to Nibi (replacing Graham in July 2025) (nibi.sharcnet.ca)
# but does not have NCBI database yet

#for now:
#graham.alliancecan.ca

#connect to graham
ssh username@graham.alliancecan.ca


#make directory for scripts or bin in your home directory
mkdir scripts
mkdir bin

#make directories for running jobs in ~/projects/def-vtai4/userid
#run jobs from this directory
cd ~/projects/def-vtai4/userid
mkdir project_name

#copy files to graham
scp local_file username@graham.alliancecan.ca:~/path_to_directory

#copy files to graham, for larger amounts of data:
scp local_file username@gra-dtn1.alliancecan.ca:~/path_to_directory

#for really large amounts of data, use Globus web-based tool




#check if commands to be used have multiple versions
#multiple available blast modules on graham
#check currently available modules
module avail blast

#to find all modules on the cluster
module spider python


#so need to set up environment by loading preferred module
#can also do this inside bash script
module load blast+/2.14.1

#FYI, cardinal is using 2.16.0+

#to unload a module
module unload blast+/2.14.1

#to reset all loaded modules back to defaults
module reset


#submit a job with commands in a bash script
#run bash script with sbatch
sbatch simple_job.sh

#if interaction is needed, run job using salloc
salloc simple_job.sh

#write the bash script in nano or plain text editor
#bash script might look like this:

#!/bin/bash
#SBATCH --time=0-30:00 # D-HH:MM
#SBATCH --account=vtai4
#SBATCH --mem=4G
#SBATCH --cpus-per-task=32
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
module load python/3.13.2
module load blast+/2.14.1
python simple_job.py 7 output_file


#or like this, if installing custom python packages within bash script, which are listed in requirements.txt
#can also locally install python packages (and not within the bash script)
#see below for more instructions on installing python packages
#or visit https://docs.alliancecan.ca/wiki/Python

#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --mem-per-cpu=1.5G      # increase as needed
#SBATCH --time=1:00:00
#SBATCH --cpus-per-task=10

module load python/3.13.2
virtualenv --no-download $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate
pip install --no-index --upgrade pip

pip install --no-index -r requirements.txt
python ...


#acceptable time formats include "minutes", "minutes:seconds", "hours:minutes:seconds", "days-hours", "days-hours:minutes" and "days-hours:minutes:seconds". Please note that the time limit will strongly affect how quickly the job is started, since longer jobs are eligible to run on fewer nodes.

#Memory may be requested with --mem-per-cpu (memory per core) or --mem (memory per node). On general-purpose (GP) clusters, a default memory amount of 256 MB per core will be allocated unless you make some other request. On Niagara, only whole nodes are allocated along with all available memory, so a memory specification is not required there.

#See the Available memory column in the Node characteristics table for each GP cluster for the Slurm specification of the maximum memory you can request on each node: BÃ©luga, Cedar, Graham, Narval.

#for multi-threaded jobs, mem means the total amount of memory (from all the threads) your code needs

#This command is needed for OpenMP jobs, but may also be needed for other kinds of threaded jobs:  export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK 

#If you want to use an entire compute node for your multithreaded job, you have to specify --cpus-per-task argument accordingly (set it to the number of CPU cores for the nodes you want to run on), and set --mem=0 (this will make the entire node memory available to the job). It is also a good idea to add #SBATCH --exclusive line, as this will ensure that no other jobs will run on your node. If you do all of the above, such a job will be submitted to a special by-node partition (as opposed to the default by-core partition), which in many cases should result in shorter queue wait time.


#NCBI databases are generally located in directories within:
/cvmfs/bio.data.computecanada.ca

#on graham
/cvmfs/bio.data.computecanada.ca/content/databases/Core/blast_dbs/2022_03_23/
#FYI, on cardinal, nr database updated May 6 2025



#where is the output?

output is placed in a file named "slurm-", suffixed with the job ID number and ".out" (e.g. slurm-123456.out), in the directory from which the job was submitted. 



#check jobs
squeue -u <username>
squeue -u <username> -t RUNNING
squeue -u <username> -t PENDING

#to get the info about jobs in the past 24 h
sacct

#short summary of the CPU and memory efficiency of a job
seff <jobid>

#cancel a job
scancel <jobid>

scancel -u <username>
scancel -t PENDING -u <username>


# to install python packages
# https://docs.alliancecan.ca/wiki/Python

#check python versions
module avail python

#load preferred version
module load python/3.13.2

#to load python scipy, numpy and related packages
module load scipy-stack

#install biopython
pip install biopython


#can create virtual environment, to install custom python packages
virtualenv --no-download VERA_PYTHON

#active virtual env
source VERA_PYTHON/bin/activate

#update pip
pip install --no-index --upgrade pip

# install python packages, e.g. biopython
pip install biopython

#to deactivate
deactivate



#to run virtual env
#load modules
#activate environment
module load python/3.13.2 scipy-stack
source VERA_PYTHON/bin/activate

#to deactivate
deactivate



#activating env from within bash script

#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --mem-per-cpu=1.5G      # increase as needed
#SBATCH --time=1:00:00

module load python/3.13.2
virtualenv --no-download $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate
pip install --no-index --upgrade pip

pip install --no-index -r requirements.txt
python ...


#needs a requirements.txt file with list of python modules to install
#create this files like this, e.g. for biopython

module load python/3.13.2
ENVDIR=/tmp/$RANDOM
virtualenv --no-download $ENVDIR
source $ENVDIR/bin/activate
pip install --no-index --upgrade pip
pip install --no-index biopython
pip freeze --local > requirements.txt
deactivate
rm -rf $ENVDIR

