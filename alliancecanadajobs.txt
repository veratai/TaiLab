#using Digital Research of Alliance Canada (Compute Canada) resources

# for help e-mail support@tech.alliancecan.ca

# should connect to Nibi (replacing Graham in July 2025) (nibi.sharcnet.ca)
# but does not have NCBI database yet

#for now:
#graham.alliancecan.ca

#connect to graham
ssh username@graham.alliancecan.ca


#SETTING UP ACCOUNT

#make directory for scripts and for bin in your home directory
mkdir scripts

#Add Vera's bin to your path to access tools needed by Tai Lab that are not installed by Alliance Canada.
#to do this, need to add /home/userid/projects/def-vtai4/bin to your PATH by editing your .bash_profile

nano .bash_profile

#under this line:
# User specific environment and startup programs

#add this text, then save (^X):
PATH=$PATH:/home/userid/projects/def-vtai4/bin

export PATH


#if you want your own bin, in your home directory
mkdir bin
#~/bin may not be in your PATH, so may need to add path in .bash_profile
#check the directories in your PATH like this:
echo $PATH
# see if you have this at the end:
/home/userid/bin
#if not, can add this to your path as well


#make directories for running jobs in ~/projects/def-vtai4/userid
#may need to make a directory named with your userid, e.g.,
cd ~/projects/def-vtai4/
mkdir vtai4

#run jobs from this directory
cd ~/projects/def-vtai4/userid
mkdir project_name

#copy files to graham
scp local_file username@graham.alliancecan.ca:~/path_to_directory

#copy files to graham, for larger amounts of data:
scp local_file username@gra-dtn1.alliancecan.ca:~/path_to_directory

#for really large amounts of data, use Globus web-based tool


#need to locally install datasets and dataformat from NCBI CLI tools
#for instructions, see section on Install using curl for Linux: #https://www.ncbi.nlm.nih.gov/datasets/docs/v2/command-line-tools/download-and-install/

#make sure datasets and dataformat are located in ~/bin (i.e. /home/userid/bin)


#NCBI databases are generally located in directories within:
/cvmfs/bio.data.computecanada.ca

#on graham
/cvmfs/bio.data.computecanada.ca/content/databases/Core/blast_dbs/2022_03_23/
#FYI, not as updated as on cardinal, where nr database updated May 6 2025



#check if available tools exist, and may have multiple versions
#multiple available blast modules on graham
#check currently available modules
module avail blast

#or, to find all modules on the cluster
module spider blast

#set up local environment by loading preferred module
#can also do this inside bash script
module load blast+/2.14.1

#FYI, cardinal is using 2.16.0+

#to unload a module
module unload blast+/2.14.1

#to reset all loaded modules back to defaults
module reset

#need to specify which python3 version to use
#load preferred version
module load python/3.13.2

#then can load or install python packages that are not available by default
#to load python scipy, numpy and related packages
module load scipy-stack

#install biopython
pip install biopython

#can also do this by creating a virtual environment
#or install within a temporary virtual environment within a bash script
#see more instructions about this below, or visit:
# https://docs.alliancecan.ca/wiki/Python



#SUBMITTING JOBS

#submit a job with commands in a bash script
#run bash script with sbatch
sbatch simple_job.sh

#if interaction is needed, run job using salloc
salloc simple_job.sh

#write the bash script in nano or plain text editor
#bash script might look like this:

#!/bin/bash
#SBATCH --time=0-30:00 # D-HH:MM
#SBATCH --account=vtai4
#SBATCH --mem=4G
#SBATCH --cpus-per-task=32
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
module load python/3.13.2
module load blast+/2.14.1
module load scipy-stack
python simple_job.py 7 output_file



#acceptable time formats include "minutes", "minutes:seconds", "hours:minutes:seconds", "days-hours", "days-hours:minutes" and "days-hours:minutes:seconds". Please note that the time limit will strongly affect how quickly the job is started, since longer jobs are eligible to run on fewer nodes.

#Memory may be requested with --mem-per-cpu (memory per core) or --mem (memory per node). On general-purpose (GP) clusters, a default memory amount of 256 MB per core will be allocated unless you make some other request. On Niagara, only whole nodes are allocated along with all available memory, so a memory specification is not required there.

#See the Available memory column in the Node characteristics table for each GP cluster for the Slurm specification of the maximum memory you can request on each node: BÃ©luga, Cedar, Graham, Narval.

#for multi-threaded jobs, mem means the total amount of memory (from all the threads) your code needs

#This command is needed for OpenMP jobs, but may also be needed for other kinds of threaded jobs:  export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK 

#If you want to use an entire compute node for your multithreaded job, you have to specify --cpus-per-task argument accordingly (set it to the number of CPU cores for the nodes you want to run on), and set --mem=0 (this will make the entire node memory available to the job). It is also a good idea to add #SBATCH --exclusive line, as this will ensure that no other jobs will run on your node. If you do all of the above, such a job will be submitted to a special by-node partition (as opposed to the default by-core partition), which in many cases should result in shorter queue wait time.


#where is the output?

output is placed in a file named "slurm-", suffixed with the job ID number and ".out" (e.g. slurm-123456.out), in the directory from which the job was submitted. 



#check jobs
squeue -u <username>
squeue -u <username> -t RUNNING
squeue -u <username> -t PENDING

#to get the info about jobs in the past 24 h
sacct

#short summary of the CPU and memory efficiency of a job
seff <jobid>

#cancel a job
scancel <jobid>

scancel -u <username>
scancel -t PENDING -u <username>


# INSTALLING PYTHON PACKAGES
# https://docs.alliancecan.ca/wiki/Python

#check python versions
module avail python

#load preferred version
module load python/3.13.2

#to load python scipy, numpy and related packages
module load scipy-stack

#install biopython
pip install biopython


#can create virtual environment, to install custom python packages
virtualenv --no-download VERA_PYTHON

#active virtual env
source VERA_PYTHON/bin/activate

#update pip
pip install --no-index --upgrade pip

# install python packages, e.g. biopython
pip install biopython

#to deactivate
deactivate



#to run virtual env
#load modules
#activate environment
module load python/3.13.2 scipy-stack
source VERA_PYTHON/bin/activate

#to deactivate
deactivate



#RUNNING JOB WITH A VIRTUAL ENVIRONMENT
#if needing custom python packages, can install within bash script, which are listed in requirements.txt
#alternatively, can also locally install python packages (and not within the bash script)
#see below for more instructions on installing python packages
#or visit https://docs.alliancecan.ca/wiki/Python

#!/bin/bash
#SBATCH --account=def-someuser
#SBATCH --mem-per-cpu=1.5G      # increase as needed
#SBATCH --time=1:00:00
#SBATCH --cpus-per-task=10

module load python/3.13.2
virtualenv --no-download $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate
pip install --no-index --upgrade pip

pip install --no-index -r requirements.txt
python ...



#needs a requirements.txt file with list of python modules to install
#create this file like this, e.g. for biopython

module load python/3.13.2
ENVDIR=/tmp/$RANDOM
virtualenv --no-download $ENVDIR
source $ENVDIR/bin/activate
pip install --no-index --upgrade pip
pip install --no-index biopython
pip freeze --local > requirements.txt
deactivate
rm -rf $ENVDIR

